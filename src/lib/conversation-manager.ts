import { chatService } from './chat';
import { ttsService } from './tts';
import { enhancedSpeechRecognitionService } from './enhanced-speech-recognition';
import { errorHandler } from './error-handler';
import { performanceMonitor } from './performance-monitor';
import { KimiMessage, ChatMessage } from '@/types';
import { usePhoneAIStore } from './store';

// æ¯ä¸ªå­—ç¬¦æ¸²æŸ“çš„é—´éš”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œè°ƒæ•´ä¸ºé€‚åˆä¸­æ–‡é˜…è¯»å’Œè¯­éŸ³åŒ¹é…çš„é€Ÿåº¦
const STREAM_TYPING_INTERVAL = 50;

export class ConversationManager {
  private static instance: ConversationManager;
  private isProcessingConversation = false;
  private currentAudioController: AbortController | null = null;
  private streamingTextTimer: NodeJS.Timeout | null = null;

  public static getInstance(): ConversationManager {
    if (!ConversationManager.instance) {
      ConversationManager.instance = new ConversationManager();
    }
    return ConversationManager.instance;
  }

  public async startVoiceConversation(): Promise<void> {
    const store = usePhoneAIStore.getState();
    
    if (!enhancedSpeechRecognitionService.isSupported()) {
      const errorMessage = errorHandler.getUserFriendlyMessage(new Error('Speech recognition not supported'));
      store.setError(errorMessage);
      return;
    }

    // Check if microphone is enabled
    if (!store.isMicrophoneEnabled) {
      console.log('Microphone is disabled, skipping voice conversation start');
      return;
    }

    // Check if already listening to prevent duplicate calls
    if (enhancedSpeechRecognitionService.getIsListening()) {
      console.log('Already listening, skipping duplicate start');
      return;
    }

    try {
      store.setRecording(true);
      store.setError(null);
      
      await performanceMonitor.monitorSpeechProcessing(
        () => enhancedSpeechRecognitionService.startListening(
          (transcript) => {
            // Handle final speech result with 4-second delay
            console.log('Received final transcript after 4-second delay:', transcript);
            this.handleUserSpeech(transcript);
          },
          (error) => {
            const userFriendlyError = errorHandler.getUserFriendlyMessage(new Error(error));
            errorHandler.handleError(new Error(error), 'Enhanced Speech Recognition');
            store.setError(userFriendlyError);
            store.setRecording(false);
            store.setWaitingToUpload(false);
          },
          () => {
            store.setRecording(true);
            console.log('Enhanced speech recognition started');
          },
          () => {
            store.setRecording(false);
            store.setWaitingToUpload(false);
            console.log('Enhanced speech recognition ended');
          },
          {
            language: store.language,
            continuous: true,
            interimResults: true,
          }
        ),
        'recognition'
      );
    } catch (error) {
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      errorHandler.handleError(error, 'Voice Conversation Start');
      store.setError(userFriendlyError);
      store.setRecording(false);
      store.setWaitingToUpload(false);
    }
  }

  public stopVoiceConversation(): void {
    enhancedSpeechRecognitionService.stopListening();
    this.stopCurrentAudio();
    this.stopStreamingText();
    const store = usePhoneAIStore.getState();
    store.setRecording(false);
    store.setProcessing(false);
    store.setSpeaking(false);
    store.setWaitingToUpload(false);
  }

  private async handleUserSpeech(transcript: string): Promise<void> {
    if (this.isProcessingConversation) {
      return; // Prevent overlapping conversations
    }

    this.isProcessingConversation = true;
    const store = usePhoneAIStore.getState();

    try {
      // Add user message to store
      store.addMessage({
        role: 'user',
        content: transcript,
      });

      store.setProcessing(true);
      store.setRecording(false);

      // Generate AI response
      const response = await this.generateAIResponse(transcript);
      
      if (response) {
        // Check if response contains </end> marker
        const hasEndMarker = response.includes('</end>');
        
        // Remove the </end> marker from the display text
        const cleanResponse = response.replace('</end>', '').trim();
        
        // Start streaming the text response
        await this.streamTextAndSpeak(cleanResponse);

        // If </end> marker was found, end the interview and generate summary
        if (hasEndMarker) {
          console.log('Interview end marker detected, generating summary...');
          setTimeout(() => {
            this.endInterviewAndSummarize();
          }, 1000); // Small delay to ensure TTS completes
        }
      }

    } catch (error) {
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      errorHandler.handleError(error, 'User Speech Processing');
      store.setError(userFriendlyError);
    } finally {
      store.setProcessing(false);
      this.isProcessingConversation = false;
    }
  }

  private async generateAIResponse(userInput: string): Promise<string | null> {
    const store = usePhoneAIStore.getState();
    
    try {
      // Convert messages to KimiMessage format
      const conversationHistory: KimiMessage[] = store.messages
        .slice(-10) // Keep last 10 messages for context
        .map(msg => ({
          role: msg.role,
          content: msg.content,
        }));

      // Add current user input
      conversationHistory.push({
        role: 'user',
        content: userInput,
      });

      const response = await performanceMonitor.monitorAPICall(
        () => chatService.sendMessage(conversationHistory),
        'kimi-chat',
        '/api/chat'
      );
      
      return response;

    } catch (error) {
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      errorHandler.handleError(error, 'AI Response Generation');
      store.setError(userFriendlyError);
      return null;
    }
  }

  // æ–°çš„æµå¼æ¸²æŸ“å’Œè¯­éŸ³æ’­æ”¾æ–¹æ³•
  private async streamTextAndSpeak(text: string): Promise<void> {
    if (!text || text.trim().length === 0) return;
    
    const store = usePhoneAIStore.getState();
    this.stopStreamingText(); // åœæ­¢ä»»ä½•æ­£åœ¨è¿›è¡Œçš„æµå¼æ¸²æŸ“
    
    try {
      store.setSpeaking(true);
      store.setPlaying(true);

      // å¼€å§‹æµå¼æ¸²æŸ“
      const messageId = store.startStreamingMessage('assistant');
      
      // åŒæ—¶è¯·æ±‚TTSéŸ³é¢‘
      const audioPromise = performanceMonitor.monitorAudioProcessing(
        () => ttsService.textToSpeech(text),
        'text-to-speech'
      );

      // å¯åŠ¨æ–‡æœ¬æµå¼æ¸²æŸ“
      const streamTextPromise = this.streamText(text);
      
      // ç­‰å¾…éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼ˆä¸ç­‰å¾…æ–‡æœ¬æµå¼æ¸²æŸ“å®Œæˆï¼‰
      this.currentAudioController = new AbortController();
      const audioBlob = await audioPromise;
      
      // éŸ³é¢‘å‡†å¤‡å¥½åç«‹å³æ’­æ”¾ï¼Œä¸æ–‡æœ¬æµå¼æ¸²æŸ“å¹¶è¡Œ
      if (this.currentAudioController && !this.currentAudioController.signal.aborted) {
        // æ’­æ”¾éŸ³é¢‘ï¼ˆä¸ç­‰å¾…streamTextå®Œæˆï¼‰
        await performanceMonitor.monitorAudioProcessing(
          () => ttsService.playAudio(audioBlob),
          'audio-playback'
        );
      }
      
      // ç¡®ä¿æ–‡æœ¬æµå¼æ¸²æŸ“å·²å®Œæˆ
      await streamTextPromise;
      
      // å®Œæˆæµå¼æ¸²æŸ“ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
      store.completeStreamingMessage();

    } catch (error) {
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      errorHandler.handleError(error, 'Text-to-Speech');
      store.setError(userFriendlyError);
      
      // ç¡®ä¿å³ä½¿å‘ç”Ÿé”™è¯¯ä¹Ÿå®Œæˆæµå¼æ¶ˆæ¯
      if (store.streamingMessage) {
        store.completeStreamingMessage();
      }
    } finally {
      store.setSpeaking(false);
      store.setPlaying(false);
      this.currentAudioController = null;
    }
  }

  // æµå¼æ¸²æŸ“æ–‡æœ¬çš„è¾…åŠ©æ–¹æ³•
  private streamText(text: string): Promise<void> {
    return new Promise<void>((resolve) => {
      const store = usePhoneAIStore.getState();
      let index = 0;
      
      // æ¸…é™¤ä»»ä½•ç°æœ‰çš„è®¡æ—¶å™¨
      if (this.streamingTextTimer) {
        clearInterval(this.streamingTextTimer);
      }
      
      // åŠ¨æ€è®¡ç®—æ‰“å­—é€Ÿåº¦ï¼Œæ ¹æ®æ–‡æœ¬é•¿åº¦ä¼˜åŒ–æ˜¾ç¤ºä½“éªŒ
      // æ–‡æœ¬è¶Šé•¿ï¼Œæ‰“å­—é€Ÿåº¦è¶Šå¿«ï¼Œä½†æœ‰æœ€å°å’Œæœ€å¤§é™åˆ¶
      const textLength = text.length;
      let typingInterval = STREAM_TYPING_INTERVAL;
      
      if (textLength > 200) {
        // å¯¹äºé•¿æ–‡æœ¬ï¼ŒåŠ å¿«é€Ÿåº¦ï¼Œä½†ä¸è¶…è¿‡æœ€å°å€¼
        typingInterval = Math.max(20, STREAM_TYPING_INTERVAL - (textLength / 20));
      }
      
      console.log(`Text length: ${textLength}, typing interval: ${typingInterval}ms`);
      
      // ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ—è¡¨ï¼Œè¿™äº›ç¬¦å·åä¼šæœ‰çŸ­æš‚åœé¡¿
      const pauseCharacters = ['ã€‚', 'ï¼Œ', 'ï¼›', 'ï¼', 'ï¼Ÿ', '.', ',', ';', '!', '?'];
      
      // è®¾ç½®è®¡æ—¶å™¨ï¼Œä¸€æ¬¡æ¸²æŸ“ä¸€ä¸ªå­—ç¬¦
      this.streamingTextTimer = setInterval(() => {
        if (index < text.length) {
          const currentChar = text[index];
          store.appendToStreamingMessage(currentChar);
          index++;
          
          // å¦‚æœå½“å‰å­—ç¬¦æ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œå¢åŠ åœé¡¿
          if (pauseCharacters.includes(currentChar)) {
            // æš‚æ—¶æ¸…é™¤è®¡æ—¶å™¨ï¼Œå¢åŠ åœé¡¿
            if (this.streamingTextTimer) {
              clearInterval(this.streamingTextTimer);
              
              // æ ¹æ®æ ‡ç‚¹ç¬¦å·ç±»å‹å†³å®šåœé¡¿æ—¶é—´
              const pauseTime = 
                ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'].includes(currentChar) 
                  ? 300  // å¥å·ç­‰åœé¡¿é•¿ä¸€äº›
                  : 150; // é€—å·ç­‰åœé¡¿çŸ­ä¸€äº›
                
              // åœé¡¿åé‡æ–°å¼€å§‹
              setTimeout(() => {
                this.streamingTextTimer = setInterval(() => {
                  if (index < text.length) {
                    store.appendToStreamingMessage(text[index]);
                    index++;
                  } else {
                    if (this.streamingTextTimer) {
                      clearInterval(this.streamingTextTimer);
                      this.streamingTextTimer = null;
                    }
                    resolve();
                  }
                }, typingInterval);
              }, pauseTime);
            }
          }
        } else {
          // å®Œæˆæ¸²æŸ“
          if (this.streamingTextTimer) {
            clearInterval(this.streamingTextTimer);
            this.streamingTextTimer = null;
          }
          resolve();
        }
      }, typingInterval);
    });
  }

  // åœæ­¢æµå¼æ–‡æœ¬æ¸²æŸ“
  private stopStreamingText(): void {
    if (this.streamingTextTimer) {
      clearInterval(this.streamingTextTimer);
      this.streamingTextTimer = null;
    }
    
    // å¦‚æœæœ‰æœªå®Œæˆçš„æµå¼æ¶ˆæ¯ï¼Œå®Œæˆå®ƒ
    const store = usePhoneAIStore.getState();
    if (store.streamingMessage && !store.streamingMessage.isComplete) {
      store.completeStreamingMessage();
    }
  }

  private stopCurrentAudio(): void {
    if (this.currentAudioController) {
      this.currentAudioController.abort();
      this.currentAudioController = null;
    }
  }

  public async sendTextMessage(text: string): Promise<void> {
    if (this.isProcessingConversation) {
      return;
    }

    this.isProcessingConversation = true;
    const store = usePhoneAIStore.getState();

    try {
      // Add user message
      store.addMessage({
        role: 'user',
        content: text,
      });

      store.setProcessing(true);

      // Generate AI response
      const response = await this.generateAIResponse(text);
      
      if (response) {
        // ä½¿ç”¨æµå¼æ¸²æŸ“æ–¹å¼å¤„ç†å“åº”
        await this.streamTextAndSpeak(response);
      }

    } catch (error) {
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      errorHandler.handleError(error, 'Text Message Processing');
      store.setError(userFriendlyError);
    } finally {
      store.setProcessing(false);
      this.isProcessingConversation = false;
    }
  }

  public isProcessing(): boolean {
    return this.isProcessingConversation;
  }

  public async continueListening(): Promise<void> {
    const store = usePhoneAIStore.getState();
    
    if (store.isCallActive && !store.isRecording && !this.isProcessingConversation && store.isMicrophoneEnabled) {
      // Wait a moment before restarting listening
      setTimeout(() => {
        const currentStore = usePhoneAIStore.getState();
        if (currentStore.isCallActive && !currentStore.isRecording && currentStore.isMicrophoneEnabled) {
          this.startVoiceConversation();
        }
      }, 1000);
    }
  }

  public toggleMicrophone(): void {
    const store = usePhoneAIStore.getState();
    const newState = !store.isMicrophoneEnabled;
    
    store.setMicrophoneEnabled(newState);
    
    if (!newState) {
      // If disabling microphone, stop current listening
      this.stopCurrentListening();
    } else if (store.isCallActive && !store.isRecording && !this.isProcessingConversation) {
      // If enabling microphone during active call, restart listening
      setTimeout(() => this.startVoiceConversation(), 500);
    }
  }

  private stopCurrentListening(): void {
    enhancedSpeechRecognitionService.stopListening();
    const store = usePhoneAIStore.getState();
    store.setRecording(false);
    store.setWaitingToUpload(false);
    store.setSilenceProgress(0);
  }

  private async endInterviewAndSummarize(): Promise<void> {
    const store = usePhoneAIStore.getState();
    
    try {
      console.log('Ending interview and generating summary...');
      
      // Stop voice conversation
      this.stopVoiceConversation();
      
      // Set processing state for summary generation
      store.setProcessing(true);
      
      // Filter out system messages and prepare for summary
      const interviewMessages = store.messages.map(msg => ({
        role: msg.role,
        content: msg.content
      }));

      console.log('Sending messages for summary:', interviewMessages.length);

      // Generate summary
      const response = await fetch('/api/interview-summary', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: interviewMessages
        })
      });

      if (!response.ok) {
        throw new Error('Failed to generate interview summary');
      }

      const summaryData = await response.json();
      
      // ä½¿ç”¨æµå¼æ¸²æŸ“æ˜¾ç¤ºæ€»ç»“
      const summaryContent = `ğŸ“‹ **é‡‡è®¿æ€»ç»“**\n\n${summaryData.summary}`;
      const messageId = store.startStreamingMessage('assistant');
      await this.streamText(summaryContent);
      store.completeStreamingMessage();

      // End the call
      store.endConversation();

      console.log('Interview summary generated successfully');

    } catch (error) {
      console.error('Error generating interview summary:', error);
      const userFriendlyError = errorHandler.getUserFriendlyMessage(error);
      store.setError(`ç”Ÿæˆé‡‡è®¿æ€»ç»“å¤±è´¥: ${userFriendlyError}`);
    } finally {
      store.setProcessing(false);
    }
  }
}

export const conversationManager = ConversationManager.getInstance();